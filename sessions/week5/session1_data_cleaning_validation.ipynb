{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cc0ea9",
   "metadata": {},
   "source": [
    "# ğŸ§¹ Week 5 â€” Session 1: Data Cleaning and Validation\n",
    "\n",
    "## Welcome to Data Cleaning!\n",
    "\n",
    "**Real-world data is messy!** Before you can analyze data, you need to clean it. This session teaches you essential data cleaning techniques that every data engineer and analyst uses daily.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- âœ… Identify common data quality issues (missing values, duplicates, wrong types)\n",
    "- âœ… Handle missing data using different strategies\n",
    "- âœ… Detect and remove duplicate records\n",
    "- âœ… Validate data types and convert when necessary\n",
    "- âœ… Apply data validation rules\n",
    "- âœ… Build reusable data cleaning functions\n",
    "\n",
    "### ğŸ“š Why Data Cleaning Matters\n",
    "\n",
    "| Problem | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| Missing Values | Calculations fail, biased results | Fill, drop, or flag |\n",
    "| Duplicates | Inflated counts, wrong totals | Detect and remove |\n",
    "| Wrong Types | Can't do math on \"123\" string | Type conversion |\n",
    "| Invalid Values | Negative ages, future dates | Validation rules |\n",
    "\n",
    "**ğŸ’¡ Data scientists spend ~80% of their time cleaning data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ee4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ SETUP: Import required libraries\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display settings for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(f\"ğŸ“¦ Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df60173",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¬ Section 1: Creating Messy Data for Practice\n",
    "\n",
    "Before we learn to clean data, let's create some realistically **messy** data to work with. This simulates what you'll encounter in the real world!\n",
    "\n",
    "### ğŸ“Š Common Data Problems We'll Include:\n",
    "- Missing values (NaN, None, empty strings)\n",
    "- Duplicate records\n",
    "- Inconsistent formatting (mixed case, extra spaces)\n",
    "- Wrong data types (numbers stored as strings)\n",
    "- Invalid values (negative ages, impossible dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Š CREATE MESSY DATA: A realistic \"dirty\" dataset\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# This is intentionally messy - just like real data!\n",
    "messy_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11],  # Note: duplicate ID!\n",
    "    'name': ['Alice', 'Bob', '  charlie', 'Charlie', 'DIANA', 'Eve', \n",
    "             'Frank', None, 'Henry', 'Ivy', 'Jack', 'alice'],  # Case issues, spaces, None\n",
    "    'email': ['alice@email.com', 'bob@email', 'charlie@email.com', 'charlie@email.com',\n",
    "              'diana@email.com', '', 'frank@email.com', 'grace@email.com',\n",
    "              'invalid-email', 'ivy@email.com', None, 'alice@email.com'],  # Invalid emails\n",
    "    'age': [25, 30, -5, 35, 28, 150, 42, 33, 27, np.nan, 45, 25],  # Negative, impossible age, NaN\n",
    "    'salary': ['50000', '60000', '55000', '55000', 'not available', '70000', \n",
    "               '80000', '65000', '45000', '90000', '75000', '50000'],  # String type, invalid\n",
    "    'signup_date': ['2023-01-15', '2023-02-20', '2023/03/10', '2023-03-10', \n",
    "                    '2023-04-05', '2023-05-12', 'invalid', '2023-07-18',\n",
    "                    '2023-08-22', '2023-09-30', '2023-10-15', '2023-01-15']  # Inconsistent format\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š MESSY DATA - Can you spot all the problems?\")\n",
    "print(\"=\" * 70)\n",
    "print(messy_data)\n",
    "print(\"\\nğŸ” Data Types:\")\n",
    "print(messy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8f638",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Section 2: Handling Missing Values\n",
    "\n",
    "Missing values are the **most common** data quality issue. Pandas represents missing values as `NaN` (Not a Number) or `None`.\n",
    "\n",
    "### ğŸ“‹ Missing Value Methods\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|---------|\n",
    "| `.isnull()` | Returns True where values are missing |\n",
    "| `.notnull()` | Returns True where values exist |\n",
    "| `.isna().sum()` | Count missing values per column |\n",
    "| `.dropna()` | Remove rows/columns with missing values |\n",
    "| `.fillna(value)` | Replace missing values with a value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7730800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš« BAD PRACTICE: Ignoring missing values\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âŒ Don't just calculate without checking for missing values!\n",
    "\n",
    "print(\"ğŸš« BAD: Ignoring missing values\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Let's try to calculate average age\n",
    "sample = pd.DataFrame({'age': [25, 30, np.nan, 35, np.nan]})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(sample)\n",
    "print(f\"\\nâ“ Average (with NaN): {sample['age'].mean()}\")  # pandas skips NaN by default\n",
    "print(\"   But wait... we lost data! How many valid values?\")\n",
    "print(f\"   Total: {len(sample)}, Valid: {sample['age'].count()}\")\n",
    "print(\"\\nâŒ Problem: We don't know WHY values are missing!\")\n",
    "print(\"   - Did the user skip the question?\")\n",
    "print(\"   - Is it a data collection error?\")\n",
    "print(\"   - Should it be 0 instead?\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa977fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ INTERMEDIATE: Handling missing values with different strategies\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Different situations need different approaches\n",
    "\n",
    "print(\"ğŸ”§ INTERMEDIATE: Missing value strategies\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', None, 'Diana'],\n",
    "    'age': [25, np.nan, 35, 28],\n",
    "    'score': [90, 85, np.nan, 95]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(sample)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Strategy 1: DROP rows with any missing values\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dropped = sample.dropna()\n",
    "print(f\"\\nğŸ“Œ Strategy 1 - dropna():\")\n",
    "print(f\"   Rows before: {len(sample)}, after: {len(dropped)}\")\n",
    "print(dropped)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Strategy 2: FILL with a specific value\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "filled_value = sample.fillna({'name': 'Unknown', 'age': 0, 'score': sample['score'].mean()})\n",
    "print(f\"\\nğŸ“Œ Strategy 2 - fillna() with specific values:\")\n",
    "print(filled_value)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Strategy 3: FILL with mean/median (for numeric columns)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "filled_mean = sample.copy()\n",
    "filled_mean['age'] = filled_mean['age'].fillna(filled_mean['age'].mean())\n",
    "filled_mean['score'] = filled_mean['score'].fillna(filled_mean['score'].median())\n",
    "print(f\"\\nğŸ“Œ Strategy 3 - fillna() with mean/median:\")\n",
    "print(filled_mean)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e687b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ¨ PYTHONIC: Comprehensive missing value handler\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Reusable function with multiple strategies\n",
    "# âœ… Reports what was done\n",
    "\n",
    "def handle_missing_values(df, strategy='report', fill_values=None):\n",
    "    \"\"\"\n",
    "    Handle missing values in a DataFrame with various strategies.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        strategy: 'report', 'drop', 'fill', or 'fill_smart'\n",
    "        fill_values: Dict of {column: fill_value} for 'fill' strategy\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame (or report if strategy='report')\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if strategy == 'report':\n",
    "        # Just report missing values\n",
    "        print(\"ğŸ“Š Missing Value Report\")\n",
    "        print(\"=\" * 40)\n",
    "        for col in df_clean.columns:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                pct = missing / len(df_clean) * 100\n",
    "                print(f\"   {col}: {missing} missing ({pct:.1f}%)\")\n",
    "        return df_clean\n",
    "    \n",
    "    elif strategy == 'drop':\n",
    "        # Drop rows with any missing values\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean.dropna()\n",
    "        print(f\"âœ… Dropped {before - len(df_clean)} rows with missing values\")\n",
    "        \n",
    "    elif strategy == 'fill':\n",
    "        # Fill with specific values\n",
    "        if fill_values:\n",
    "            df_clean = df_clean.fillna(fill_values)\n",
    "            print(f\"âœ… Filled missing values with specified values\")\n",
    "            \n",
    "    elif strategy == 'fill_smart':\n",
    "        # Smart fill: mean for numeric, mode for categorical\n",
    "        for col in df_clean.columns:\n",
    "            if df_clean[col].isnull().any():\n",
    "                if df_clean[col].dtype in ['float64', 'int64']:\n",
    "                    fill_val = df_clean[col].mean()\n",
    "                    df_clean[col] = df_clean[col].fillna(fill_val)\n",
    "                    print(f\"   {col}: filled with mean ({fill_val:.2f})\")\n",
    "                else:\n",
    "                    fill_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "                    df_clean[col] = df_clean[col].fillna(fill_val)\n",
    "                    print(f\"   {col}: filled with mode ('{fill_val}')\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"âœ¨ PYTHONIC: Smart missing value handling\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample data with mixed types\n",
    "sample = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', None, 'Diana', 'Eve'],\n",
    "    'age': [25, np.nan, 35, 28, np.nan],\n",
    "    'department': ['Sales', 'IT', 'Sales', None, 'IT']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nğŸ” Report:\")\n",
    "handle_missing_values(sample, strategy='report')\n",
    "\n",
    "print(\"\\nğŸ§  Smart Fill:\")\n",
    "cleaned = handle_missing_values(sample, strategy='fill_smart')\n",
    "print(\"\\nResult:\")\n",
    "print(cleaned)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132a7ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Section 3: Handling Duplicates\n",
    "\n",
    "Duplicate records can **inflate your counts** and **skew your analysis**. Always check for and handle duplicates!\n",
    "\n",
    "### ğŸ“‹ Duplicate Detection Methods\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|---------|\n",
    "| `.duplicated()` | Returns True for duplicate rows |\n",
    "| `.duplicated(subset=[cols])` | Check specific columns for duplicates |\n",
    "| `.drop_duplicates()` | Remove duplicate rows |\n",
    "| `.drop_duplicates(keep='first')` | Keep first occurrence |\n",
    "| `.drop_duplicates(keep='last')` | Keep last occurrence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš« BAD PRACTICE: Not checking for duplicates\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸš« BAD: Ignoring duplicates\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Look at our messy data\n",
    "df = messy_data.copy()\n",
    "\n",
    "print(\"ğŸ“Š Customer data (look for duplicates!):\")\n",
    "print(df[['customer_id', 'name', 'email']].head(8))\n",
    "\n",
    "# BAD: calculating totals without checking duplicates\n",
    "print(f\"\\nâŒ Total customers (wrong!): {len(df)}\")\n",
    "print(\"   But customer_id 3 appears twice!\")\n",
    "print(\"   This inflates our count and any derived metrics!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa116ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ INTERMEDIATE: Removing duplicates with options\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Choose which duplicate to keep\n",
    "\n",
    "print(\"ğŸ”§ INTERMEDIATE: Removing duplicates\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "print(f\"Records before: {len(df)}\")\n",
    "\n",
    "# Remove duplicates based on customer_id, keep FIRST occurrence\n",
    "df_clean = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "print(f\"Records after (keep='first'): {len(df_clean)}\")\n",
    "\n",
    "# Alternative: keep LAST occurrence\n",
    "df_last = df.drop_duplicates(subset=['customer_id'], keep='last')\n",
    "print(f\"Records after (keep='last'): {len(df_last)}\")\n",
    "\n",
    "# Show what was removed\n",
    "print(\"\\nğŸ” Removed record (keeping first):\")\n",
    "removed = df[~df.index.isin(df_clean.index)]\n",
    "print(removed[['customer_id', 'name', 'email']])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ¨ PYTHONIC: Complete duplicate handling function\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Comprehensive duplicate management\n",
    "\n",
    "def handle_duplicates(df, subset=None, keep='first', report=True):\n",
    "    \"\"\"\n",
    "    Handle duplicates in a DataFrame with detailed reporting.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        subset: Columns to check for duplicates (None = all columns)\n",
    "        keep: 'first', 'last', or False (drop all duplicates)\n",
    "        report: Whether to print a report\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = df_clean.duplicated(subset=subset, keep=False)\n",
    "    dup_count = duplicates.sum()\n",
    "    \n",
    "    if report:\n",
    "        print(\"ğŸ“Š Duplicate Report\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"   Total records: {len(df_clean)}\")\n",
    "        print(f\"   Duplicate records: {dup_count}\")\n",
    "        \n",
    "        if dup_count > 0:\n",
    "            print(f\"\\nğŸ” Duplicate groups:\")\n",
    "            dup_df = df_clean[duplicates]\n",
    "            if subset:\n",
    "                print(dup_df.groupby(subset).size())\n",
    "            else:\n",
    "                print(f\"   {dup_count} exact duplicates found\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=subset, keep=keep)\n",
    "    \n",
    "    removed = len(df) - len(df_clean)\n",
    "    if report:\n",
    "        print(f\"\\nâœ… Removed {removed} duplicate(s), keeping '{keep}'\")\n",
    "        print(f\"   Final records: {len(df_clean)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"âœ¨ PYTHONIC: Comprehensive duplicate handling\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "\n",
    "# Use our function\n",
    "df_clean = handle_duplicates(df, subset=['customer_id'], keep='first')\n",
    "print(\"\\nğŸ“Š Clean data preview:\")\n",
    "print(df_clean[['customer_id', 'name', 'email']].head())\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d9b60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¤ Section 4: String Cleaning and Standardization\n",
    "\n",
    "Text data often has inconsistent formatting: extra spaces, mixed case, typos. Let's clean it up!\n",
    "\n",
    "### ğŸ“‹ String Cleaning Methods\n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|---------|\n",
    "| `.str.strip()` | Remove leading/trailing spaces | `\"  Alice  \"` â†’ `\"Alice\"` |\n",
    "| `.str.lower()` | Convert to lowercase | `\"ALICE\"` â†’ `\"alice\"` |\n",
    "| `.str.upper()` | Convert to uppercase | `\"alice\"` â†’ `\"ALICE\"` |\n",
    "| `.str.title()` | Title case | `\"alice smith\"` â†’ `\"Alice Smith\"` |\n",
    "| `.str.replace()` | Replace patterns | Remove special characters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš« BAD PRACTICE: Comparing strings without standardization\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸš« BAD: String comparison without standardization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "\n",
    "# Try to find all \"Alice\" entries\n",
    "alice_count = (df['name'] == 'Alice').sum()\n",
    "print(f\"Looking for 'Alice': found {alice_count}\")\n",
    "\n",
    "print(\"\\nğŸ” But look at the actual names:\")\n",
    "print(df['name'].unique())\n",
    "print(\"\\nâŒ Problems:\")\n",
    "print(\"   - '  charlie' has leading spaces\")\n",
    "print(\"   - 'DIANA' is uppercase\")\n",
    "print(\"   - 'alice' is lowercase\")\n",
    "print(\"   - These are the SAME people but won't match!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42858729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ¨ PYTHONIC: Comprehensive string cleaning function\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Chain all operations elegantly\n",
    "\n",
    "def clean_string_column(series, case='title'):\n",
    "    \"\"\"\n",
    "    Clean a string column with comprehensive standardization.\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series (column) to clean\n",
    "        case: 'title', 'lower', 'upper', or None\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned Series\n",
    "    \"\"\"\n",
    "    # Start with a copy\n",
    "    cleaned = series.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    cleaned = cleaned.fillna('')\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    cleaned = cleaned.str.strip()\n",
    "    \n",
    "    # Remove multiple spaces (replace multiple spaces with single)\n",
    "    cleaned = cleaned.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Apply case transformation\n",
    "    if case == 'title':\n",
    "        cleaned = cleaned.str.title()\n",
    "    elif case == 'lower':\n",
    "        cleaned = cleaned.str.lower()\n",
    "    elif case == 'upper':\n",
    "        cleaned = cleaned.str.upper()\n",
    "    \n",
    "    # Replace empty strings with NaN (so we know they're missing)\n",
    "    cleaned = cleaned.replace('', np.nan)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "print(\"âœ¨ PYTHONIC: Clean string function\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "\n",
    "# Clean the name column\n",
    "df['name_clean'] = clean_string_column(df['name'], case='title')\n",
    "\n",
    "# Now find all Alice entries\n",
    "alice_count = (df['name_clean'] == 'Alice').sum()\n",
    "\n",
    "print(\"ğŸ“Š Before vs After cleaning:\")\n",
    "print(df[['name', 'name_clean']])\n",
    "print(f\"\\nâœ… Found {alice_count} 'Alice' entries after cleaning!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47a897",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Section 5: Data Validation\n",
    "\n",
    "Validation ensures your data makes sense! Catch impossible values like negative ages or invalid emails.\n",
    "\n",
    "### ğŸ“‹ Common Validation Rules\n",
    "\n",
    "| Data Type | Validation Rule | Example |\n",
    "|-----------|-----------------|---------|\n",
    "| Age | Must be 0-120 | `age >= 0 and age <= 120` |\n",
    "| Email | Must contain @ | `'@' in email` |\n",
    "| Date | Must be valid format | Use `pd.to_datetime()` |\n",
    "| Percentage | Must be 0-100 | `pct >= 0 and pct <= 100` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ffc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš« BAD PRACTICE: No validation - garbage in, garbage out!\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸš« BAD: No data validation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "\n",
    "print(\"ğŸ“Š Look at the 'age' column:\")\n",
    "print(df[['name', 'age']].head(8))\n",
    "print(\"\\nâŒ Problems found:\")\n",
    "print(\"   - Negative age: -5 (impossible!)\")\n",
    "print(\"   - Age 150 (impossible!)\")\n",
    "print(\"   - NaN value (missing)\")\n",
    "print(\"\\nâŒ If we calculate average age without validation:\")\n",
    "print(f\"   Average: {df['age'].mean():.2f} (skewed by bad data!)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ¨ PYTHONIC: Comprehensive data validation framework\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âœ… Reusable validation with detailed reporting\n",
    "\n",
    "import re\n",
    "\n",
    "def validate_dataframe(df, rules):\n",
    "    \"\"\"\n",
    "    Validate a DataFrame against a set of rules.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        rules: Dict of {column: validation_function}\n",
    "               Each function should return a boolean Series\n",
    "               \n",
    "    Returns:\n",
    "        Dict with validation results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'valid_rows': len(df),\n",
    "        'issues': [],\n",
    "        'details': {}\n",
    "    }\n",
    "    \n",
    "    for column, rule_func in rules.items():\n",
    "        if column not in df.columns:\n",
    "            results['issues'].append(f\"Column '{column}' not found\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            is_valid = rule_func(df[column])\n",
    "            invalid_count = (~is_valid).sum()\n",
    "            \n",
    "            if invalid_count > 0:\n",
    "                results['issues'].append(f\"{column}: {invalid_count} invalid values\")\n",
    "                results['details'][column] = {\n",
    "                    'invalid_count': invalid_count,\n",
    "                    'invalid_indices': df[~is_valid].index.tolist()\n",
    "                }\n",
    "        except Exception as e:\n",
    "            results['issues'].append(f\"{column}: validation error - {e}\")\n",
    "    \n",
    "    results['valid_rows'] = len(df) - len(results['issues'])\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define validation rules\n",
    "validation_rules = {\n",
    "    'age': lambda x: (x >= 0) & (x <= 120) | x.isna(),  # Valid age or missing\n",
    "    'email': lambda x: x.str.contains(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', regex=True, na=False) | x.isna(),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"âœ¨ PYTHONIC: Data validation framework\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df = messy_data.copy()\n",
    "results = validate_dataframe(df, validation_rules)\n",
    "\n",
    "print(\"ğŸ“‹ Validation Report:\")\n",
    "print(\"=\" * 40)\n",
    "if results['issues']:\n",
    "    print(\"âŒ Issues found:\")\n",
    "    for issue in results['issues']:\n",
    "        print(f\"   â€¢ {issue}\")\n",
    "else:\n",
    "    print(\"âœ… All validations passed!\")\n",
    "\n",
    "# Show invalid records\n",
    "print(\"\\nğŸ“Š Records with invalid data:\")\n",
    "for col, details in results['details'].items():\n",
    "    invalid_idx = details['invalid_indices']\n",
    "    print(f\"\\n{col} issues:\")\n",
    "    print(df.loc[invalid_idx, ['name', col]])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747f685",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Section 6: Interactive Practice Activities\n",
    "\n",
    "Now it's your turn to practice data cleaning!\n",
    "\n",
    "### ğŸ‹ï¸ Activity 1: Clean the Customer Data\n",
    "Apply all the cleaning techniques you learned to clean the messy_data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¯ ACTIVITY 1: Build a Complete Data Cleaning Pipeline\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TODO: Clean the messy_data DataFrame step by step\n",
    "\n",
    "# Start with original messy data\n",
    "df = messy_data.copy()\n",
    "\n",
    "print(\"ğŸ“Š STARTING DATA:\")\n",
    "print(df.head())\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 1: Remove duplicates\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TODO: Remove duplicate customer_ids, keep first occurrence\n",
    "df = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "print(f\"\\nâœ… Step 1: Removed duplicates. Shape: {df.shape}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 2: Clean name column\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TODO: Strip whitespace and convert to title case\n",
    "df['name'] = df['name'].fillna('').str.strip().str.title()\n",
    "df['name'] = df['name'].replace('', np.nan)  # Convert empty to NaN\n",
    "print(f\"âœ… Step 2: Cleaned names\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 3: Fix age column\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TODO: Replace invalid ages (negative or > 120) with NaN\n",
    "df.loc[(df['age'] < 0) | (df['age'] > 120), 'age'] = np.nan\n",
    "print(f\"âœ… Step 3: Fixed invalid ages\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 4: Convert salary to numeric\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TODO: Convert salary column from string to number\n",
    "df['salary'] = pd.to_numeric(df['salary'], errors='coerce')\n",
    "print(f\"âœ… Step 4: Converted salary to numeric\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FINAL REPORT\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“Š CLEANED DATA:\")\n",
    "print(df)\n",
    "print(f\"\\nğŸ“ˆ Summary:\")\n",
    "print(f\"   Rows: {len(df)}\")\n",
    "print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   Average age: {df['age'].mean():.1f}\")\n",
    "print(f\"   Average salary: ${df['salary'].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482683e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Section 7: Key Takeaways & Summary\n",
    "\n",
    "### ğŸ¯ What You Learned Today\n",
    "\n",
    "| Concept | Key Methods |\n",
    "|---------|-------------|\n",
    "| **Missing Values** | `isnull()`, `fillna()`, `dropna()` |\n",
    "| **Duplicates** | `duplicated()`, `drop_duplicates()` |\n",
    "| **String Cleaning** | `str.strip()`, `str.lower()`, `str.title()` |\n",
    "| **Validation** | Boolean indexing, custom rules |\n",
    "| **Type Conversion** | `pd.to_numeric()`, `pd.to_datetime()` |\n",
    "\n",
    "### ğŸ’¡ Best Practices Checklist\n",
    "\n",
    "âœ… Always check for missing values first with `df.isnull().sum()`  \n",
    "âœ… Check for duplicates before analysis  \n",
    "âœ… Standardize text columns (strip, case)  \n",
    "âœ… Validate numeric ranges make sense  \n",
    "âœ… Convert data types before calculations  \n",
    "âœ… Keep original data - work on copies!  \n",
    "\n",
    "### ğŸ”® What's Next?\n",
    "\n",
    "In the next session, we'll work in groups to:\n",
    "- Clean a real-world messy dataset together\n",
    "- Build a complete data cleaning pipeline\n",
    "- Practice debugging data quality issues\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ† Great job completing Week 5 Session 1!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸŒ Week 8 â€” Group Session: Introduction to Web Scraping\n",
        "\n",
        "## ğŸ¯ Session Overview\n",
        "\n",
        "Welcome to the **Web Scraping Group Session**! Today you'll work together to build practical web scraping skills by extracting data from HTML documents.\n",
        "\n",
        "### ğŸ‘¥ Group Roles\n",
        "\n",
        "| Role | Responsibility |\n",
        "|------|----------------|\n",
        "| ğŸ“¥ **Fetcher** | Write code to retrieve HTML content |\n",
        "| ğŸ” **Parser** | Build the HTMLParser class |\n",
        "| âœ… **Validator** | Check output correctness, handle edge cases |\n",
        "| ğŸ“ **Documenter** | Add comments, explain the logic |\n",
        "\n",
        "*Rotate roles after each activity!*\n",
        "\n",
        "### ğŸ“‹ Today's Activities\n",
        "\n",
        "1. **Link Classification** - Parse and categorize URLs\n",
        "2. **Product Extraction** - Extract structured data\n",
        "3. **News Headlines** - Build a headline scraper\n",
        "4. **Ethics Discussion** - Responsible scraping practices\n",
        "\n",
        "### ğŸ¯ Skills You'll Practice\n",
        "\n",
        "| Skill | Description |\n",
        "|-------|-------------|\n",
        "| `HTMLParser` | Event-driven HTML parsing |\n",
        "| `handle_starttag()` | Processing opening tags |\n",
        "| `handle_data()` | Capturing text content |\n",
        "| URL Analysis | Absolute vs relative URLs |\n",
        "| Error Handling | Robust data extraction |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "\n",
        "- âœ… **Collaborate** on fetching and parsing HTML documents\n",
        "- âœ… **Extract** and classify different types of links\n",
        "- âœ… **Parse** structured data (products, articles, etc.)\n",
        "- âœ… **Discuss** ethical considerations and rate limiting\n",
        "- âœ… **Build** reusable parser components\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¥ Setup: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“¦ SETUP: Import Required Libraries\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n",
        "print()\n",
        "print(\"ğŸ“š Tools available:\")\n",
        "print(\"  â€¢ HTMLParser - Parse HTML documents\")\n",
        "print(\"  â€¢ urljoin - Combine base URL with relative path\")\n",
        "print(\"  â€¢ urlparse - Break URL into components\")\n",
        "print()\n",
        "print(\"ğŸ¯ Ready for group activities!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”— Activity 1: Link Classification\n",
        "\n",
        "### Scenario\n",
        "A website owner wants to analyze their site's links to find:\n",
        "- How many are internal (same domain)\n",
        "- How many are external (other domains)\n",
        "- How many are relative (need base URL)\n",
        "\n",
        "### Discussion Questions (5 minutes)\n",
        "1. **How do you identify an absolute URL?** (starts with http:// or https://)\n",
        "2. **What makes a URL \"relative\"?** (starts with /, ../, or just a path)\n",
        "3. **Why would a website use relative URLs?** (portability, shorter)\n",
        "\n",
        "### The Starting Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸš« STARTING POINT: Basic link extraction (no classification)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "sample_html = '''\n",
        "<html>\n",
        "<head><base href=\"https://mysite.com/\"></head>\n",
        "<body>\n",
        "    <nav>\n",
        "        <a href=\"https://mysite.com/home\">Home</a>\n",
        "        <a href=\"/about\">About</a>\n",
        "        <a href=\"products/list\">Products</a>\n",
        "        <a href=\"../help\">Help</a>\n",
        "        <a href=\"https://google.com\">Google</a>\n",
        "        <a href=\"https://github.com/python\">Python GitHub</a>\n",
        "        <a href=\"#section1\">Jump to Section</a>\n",
        "        <a href=\"mailto:contact@mysite.com\">Email Us</a>\n",
        "    </nav>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class BasicLinkParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.links = []\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        if tag == 'a':\n",
        "            for k, v in attrs:\n",
        "                if k == 'href':\n",
        "                    self.links.append(v)\n",
        "\n",
        "p = BasicLinkParser()\n",
        "p.feed(sample_html)\n",
        "print(\"ğŸ”— All links found:\")\n",
        "for link in p.links:\n",
        "    print(f\"  â€¢ {link}\")\n",
        "\n",
        "# âš ï¸ GROUP DISCUSSION: This just lists links!\n",
        "# How do we classify them as internal/external/relative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¯ Your Task: Classify and Count Links\n",
        "\n",
        "**Requirements:**\n",
        "1. Classify each link as:\n",
        "   - ğŸ  **Internal** - Same domain (mysite.com)\n",
        "   - ğŸŒ **External** - Different domain\n",
        "   - ğŸ“„ **Relative** - Starts with / or ../ or no protocol\n",
        "   - âš“ **Anchor** - Starts with #\n",
        "   - ğŸ“§ **Special** - mailto:, tel:, javascript:, etc.\n",
        "2. Count each category\n",
        "3. Convert relative URLs to absolute using the base URL\n",
        "\n",
        "**Hints:**\n",
        "```python\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# Parse URL into components\n",
        "parsed = urlparse(\"https://example.com/path?query=1\")\n",
        "# parsed.scheme = \"https\"\n",
        "# parsed.netloc = \"example.com\"\n",
        "# parsed.path = \"/path\"\n",
        "\n",
        "# Join base URL with relative path\n",
        "full_url = urljoin(\"https://mysite.com/\", \"/about\")\n",
        "# Result: \"https://mysite.com/about\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ± GROUP WORKSPACE: Link Classifier\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Work together to classify all the links!\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "BASE_URL = \"https://mysite.com/\"\n",
        "\n",
        "sample_html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <nav>\n",
        "        <a href=\"https://mysite.com/home\">Home</a>\n",
        "        <a href=\"/about\">About</a>\n",
        "        <a href=\"products/list\">Products</a>\n",
        "        <a href=\"../help\">Help</a>\n",
        "        <a href=\"https://google.com\">Google</a>\n",
        "        <a href=\"https://github.com/python\">Python GitHub</a>\n",
        "        <a href=\"#section1\">Jump to Section</a>\n",
        "        <a href=\"mailto:contact@mysite.com\">Email Us</a>\n",
        "    </nav>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# TODO: Create a LinkClassifier that:\n",
        "# 1. Extracts all links\n",
        "# 2. Classifies each as internal/external/relative/anchor/special\n",
        "# 3. Converts relative to absolute\n",
        "# 4. Counts each category\n",
        "\n",
        "# Your code here:\n",
        "pass  # Replace with your implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ¨ SOLUTION: Comprehensive Link Classifier\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Uncomment to see the solution!\n",
        "\n",
        "'''\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "class LinkClassifier(HTMLParser):\n",
        "    def __init__(self, base_url):\n",
        "        super().__init__()\n",
        "        self.base_url = base_url\n",
        "        self.base_domain = urlparse(base_url).netloc\n",
        "        self.links = {\n",
        "            'internal': [],\n",
        "            'external': [],\n",
        "            'relative': [],\n",
        "            'anchor': [],\n",
        "            'special': []\n",
        "        }\n",
        "    \n",
        "    def classify_link(self, href):\n",
        "        \"\"\"Classify a link and return (category, full_url).\"\"\"\n",
        "        if not href:\n",
        "            return None, None\n",
        "        \n",
        "        # Anchors\n",
        "        if href.startswith('#'):\n",
        "            return 'anchor', href\n",
        "        \n",
        "        # Special protocols\n",
        "        if href.startswith(('mailto:', 'tel:', 'javascript:', 'data:')):\n",
        "            return 'special', href\n",
        "        \n",
        "        # Absolute URLs\n",
        "        if href.startswith(('http://', 'https://')):\n",
        "            parsed = urlparse(href)\n",
        "            if parsed.netloc == self.base_domain:\n",
        "                return 'internal', href\n",
        "            else:\n",
        "                return 'external', href\n",
        "        \n",
        "        # Relative URLs - convert to absolute\n",
        "        full_url = urljoin(self.base_url, href)\n",
        "        return 'relative', full_url\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        if tag == 'a':\n",
        "            for name, value in attrs:\n",
        "                if name == 'href':\n",
        "                    category, url = self.classify_link(value)\n",
        "                    if category:\n",
        "                        self.links[category].append({\n",
        "                            'original': value,\n",
        "                            'resolved': url\n",
        "                        })\n",
        "    \n",
        "    def get_summary(self):\n",
        "        \"\"\"Return classification summary.\"\"\"\n",
        "        return {cat: len(links) for cat, links in self.links.items()}\n",
        "\n",
        "# Test it!\n",
        "BASE_URL = \"https://mysite.com/\"\n",
        "sample_html = \"\"\" (same HTML as before) \"\"\"\n",
        "\n",
        "classifier = LinkClassifier(BASE_URL)\n",
        "classifier.feed(\"\"\"\n",
        "<html>\n",
        "<body>\n",
        "    <nav>\n",
        "        <a href=\"https://mysite.com/home\">Home</a>\n",
        "        <a href=\"/about\">About</a>\n",
        "        <a href=\"products/list\">Products</a>\n",
        "        <a href=\"../help\">Help</a>\n",
        "        <a href=\"https://google.com\">Google</a>\n",
        "        <a href=\"https://github.com/python\">Python GitHub</a>\n",
        "        <a href=\"#section1\">Jump to Section</a>\n",
        "        <a href=\"mailto:contact@mysite.com\">Email Us</a>\n",
        "    </nav>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# Display results\n",
        "print(\"ğŸ”— Link Classification Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "emojis = {'internal': 'ğŸ ', 'external': 'ğŸŒ', 'relative': 'ğŸ“„', \n",
        "          'anchor': 'âš“', 'special': 'ğŸ“§'}\n",
        "\n",
        "for category, links in classifier.links.items():\n",
        "    if links:\n",
        "        print(f\"\\\\n{emojis[category]} {category.upper()} ({len(links)}):\")\n",
        "        for link in links:\n",
        "            print(f\"   {link['original']}\")\n",
        "            if link['original'] != link['resolved']:\n",
        "                print(f\"   â†’ {link['resolved']}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š Summary:\", classifier.get_summary())\n",
        "'''\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the solution above to see it in action!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ›’ Activity 2: Product Data Extraction\n",
        "\n",
        "### Scenario\n",
        "An e-commerce site has product listings. Your team needs to extract:\n",
        "- Product name\n",
        "- Price (as a number, not string)\n",
        "- Rating (if available)\n",
        "- Availability status\n",
        "\n",
        "### Discussion Questions (5 minutes)\n",
        "1. **Why use HTMLParser instead of string methods?** (robustness)\n",
        "2. **How do we track \"where we are\" in the HTML?** (state variables)\n",
        "3. **How do we convert \"$19.99\" to 19.99?** (strip $ and convert to float)\n",
        "\n",
        "### The Starting Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸš« STARTING POINT: String-based extraction (fragile!)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "product_html = '''\n",
        "<div class=\"products\">\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Wireless Earbuds</h3>\n",
        "        <span class=\"price\">$79.99</span>\n",
        "        <span class=\"rating\">4.5 stars</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Smart Watch Pro</h3>\n",
        "        <span class=\"price\">$299.00</span>\n",
        "        <span class=\"rating\">4.8 stars</span>\n",
        "        <span class=\"stock out-of-stock\">Out of Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">USB-C Hub</h3>\n",
        "        <span class=\"price\">$45.50</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Fragile string extraction - DON'T DO THIS!\n",
        "def extract_between(s, start, end):\n",
        "    i = s.find(start)\n",
        "    if i == -1:\n",
        "        return ''\n",
        "    j = s.find(end, i + len(start))\n",
        "    if j == -1:\n",
        "        return ''\n",
        "    return s[i + len(start):j]\n",
        "\n",
        "# Only finds the first one!\n",
        "name = extract_between(product_html, '<h3 class=\"name\">', '</h3>')\n",
        "price = extract_between(product_html, '<span class=\"price\">', '</span>')\n",
        "\n",
        "print(\"ğŸš« String-based extraction:\")\n",
        "print(f\"  Name: {name}\")\n",
        "print(f\"  Price: {price}\")\n",
        "print()\n",
        "print(\"âš ï¸ Problems:\")\n",
        "print(\"  âŒ Only finds first product\")\n",
        "print(\"  âŒ Breaks if HTML structure changes\")\n",
        "print(\"  âŒ Can't handle nested tags\")\n",
        "print(\"  âŒ No validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¯ Your Task: Build a Product Parser\n",
        "\n",
        "**Requirements:**\n",
        "1. Extract ALL products (not just the first)\n",
        "2. For each product, extract:\n",
        "   - Name (from h3.name)\n",
        "   - Price as a float (strip $ sign)\n",
        "   - Rating as a float if present (strip \"stars\")\n",
        "   - Stock status (in-stock or out-of-stock)\n",
        "3. Return a list of product dictionaries\n",
        "\n",
        "**Hints:**\n",
        "```python\n",
        "# Convert price string to float\n",
        "price_str = \"$79.99\"\n",
        "price_float = float(price_str.replace('$', ''))  # 79.99\n",
        "\n",
        "# Extract number from rating\n",
        "rating_str = \"4.5 stars\"\n",
        "rating_float = float(rating_str.split()[0])  # 4.5\n",
        "\n",
        "# Check class for stock status\n",
        "# class=\"stock in-stock\" â†’ available\n",
        "# class=\"stock out-of-stock\" â†’ unavailable\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ± GROUP WORKSPACE: Product Parser\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "product_html = '''\n",
        "<div class=\"products\">\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Wireless Earbuds</h3>\n",
        "        <span class=\"price\">$79.99</span>\n",
        "        <span class=\"rating\">4.5 stars</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Smart Watch Pro</h3>\n",
        "        <span class=\"price\">$299.00</span>\n",
        "        <span class=\"rating\">4.8 stars</span>\n",
        "        <span class=\"stock out-of-stock\">Out of Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">USB-C Hub</h3>\n",
        "        <span class=\"price\">$45.50</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# TODO: Create a ProductParser class that:\n",
        "# 1. Finds all products\n",
        "# 2. Extracts name, price, rating, stock status\n",
        "# 3. Converts price/rating to numbers\n",
        "# 4. Stores as list of dictionaries\n",
        "\n",
        "# Your code here:\n",
        "pass  # Replace with your implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ¨ SOLUTION: Comprehensive Product Parser\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Uncomment to see the solution!\n",
        "\n",
        "'''\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class ProductParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.products = []\n",
        "        self.current_product = None\n",
        "        self.current_field = None\n",
        "        self.in_stock = None\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs_dict = dict(attrs)\n",
        "        class_name = attrs_dict.get('class', '')\n",
        "        \n",
        "        # Start of a new product\n",
        "        if tag == 'div' and 'product' in class_name and 'products' not in class_name:\n",
        "            self.current_product = {\n",
        "                'name': None,\n",
        "                'price': None,\n",
        "                'rating': None,\n",
        "                'in_stock': None\n",
        "            }\n",
        "        \n",
        "        # Field detection\n",
        "        elif self.current_product is not None:\n",
        "            if tag == 'h3' and 'name' in class_name:\n",
        "                self.current_field = 'name'\n",
        "            elif tag == 'span' and 'price' in class_name:\n",
        "                self.current_field = 'price'\n",
        "            elif tag == 'span' and 'rating' in class_name:\n",
        "                self.current_field = 'rating'\n",
        "            elif tag == 'span' and 'stock' in class_name:\n",
        "                self.current_field = 'stock'\n",
        "                # Check the class for stock status\n",
        "                self.current_product['in_stock'] = 'in-stock' in class_name\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        if self.current_field and data.strip():\n",
        "            text = data.strip()\n",
        "            \n",
        "            if self.current_field == 'name':\n",
        "                self.current_product['name'] = text\n",
        "            \n",
        "            elif self.current_field == 'price':\n",
        "                # Convert \"$79.99\" to 79.99\n",
        "                try:\n",
        "                    self.current_product['price'] = float(text.replace('$', '').replace(',', ''))\n",
        "                except ValueError:\n",
        "                    self.current_product['price'] = 0.0\n",
        "            \n",
        "            elif self.current_field == 'rating':\n",
        "                # Convert \"4.5 stars\" to 4.5\n",
        "                try:\n",
        "                    self.current_product['rating'] = float(text.split()[0])\n",
        "                except (ValueError, IndexError):\n",
        "                    self.current_product['rating'] = None\n",
        "            \n",
        "            self.current_field = None\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        # End of product div\n",
        "        if tag == 'div' and self.current_product and self.current_product.get('name'):\n",
        "            self.products.append(self.current_product)\n",
        "            self.current_product = None\n",
        "\n",
        "# Parse products\n",
        "parser = ProductParser()\n",
        "parser.feed(\"\"\"\n",
        "<div class=\"products\">\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Wireless Earbuds</h3>\n",
        "        <span class=\"price\">$79.99</span>\n",
        "        <span class=\"rating\">4.5 stars</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">Smart Watch Pro</h3>\n",
        "        <span class=\"price\">$299.00</span>\n",
        "        <span class=\"rating\">4.8 stars</span>\n",
        "        <span class=\"stock out-of-stock\">Out of Stock</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h3 class=\"name\">USB-C Hub</h3>\n",
        "        <span class=\"price\">$45.50</span>\n",
        "        <span class=\"stock in-stock\">In Stock</span>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "# Display results\n",
        "print(\"ğŸ›’ Products Extracted\")\n",
        "print(\"=\" * 60)\n",
        "for i, product in enumerate(parser.products, 1):\n",
        "    stock_icon = \"âœ…\" if product['in_stock'] else \"âŒ\"\n",
        "    rating_str = f\"â­ {product['rating']}\" if product['rating'] else \"No rating\"\n",
        "    print(f\"\\\\n{i}. {product['name']}\")\n",
        "    print(f\"   ğŸ’° ${product['price']:.2f}\")\n",
        "    print(f\"   {rating_str}\")\n",
        "    print(f\"   {stock_icon} {'In Stock' if product['in_stock'] else 'Out of Stock'}\")\n",
        "\n",
        "# Summary stats\n",
        "print(\"\\\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“Š Summary:\")\n",
        "available = sum(1 for p in parser.products if p['in_stock'])\n",
        "avg_price = sum(p['price'] for p in parser.products) / len(parser.products)\n",
        "print(f\"   Total products: {len(parser.products)}\")\n",
        "print(f\"   In stock: {available}\")\n",
        "print(f\"   Average price: ${avg_price:.2f}\")\n",
        "'''\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the solution above to see it in action!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“° Activity 3: News Headlines Scraper\n",
        "\n",
        "### Scenario\n",
        "Build a parser that extracts news headlines with their metadata (source, date, category)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“° NEWS HEADLINES DATA\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "news_html = '''\n",
        "<div class=\"news-feed\">\n",
        "    <article class=\"news-item\" data-category=\"technology\">\n",
        "        <h2 class=\"headline\"><a href=\"/article/1\">Python 4.0 Released with Major Features</a></h2>\n",
        "        <span class=\"source\">Tech Daily</span>\n",
        "        <time datetime=\"2024-01-15\">January 15, 2024</time>\n",
        "        <p class=\"summary\">The Python Software Foundation announces...</p>\n",
        "    </article>\n",
        "    \n",
        "    <article class=\"news-item\" data-category=\"business\">\n",
        "        <h2 class=\"headline\"><a href=\"/article/2\">AI Startups See Record Funding</a></h2>\n",
        "        <span class=\"source\">Business Weekly</span>\n",
        "        <time datetime=\"2024-01-14\">January 14, 2024</time>\n",
        "        <p class=\"summary\">Venture capital firms invested over $10B...</p>\n",
        "    </article>\n",
        "    \n",
        "    <article class=\"news-item\" data-category=\"technology\">\n",
        "        <h2 class=\"headline\"><a href=\"/article/3\">New Cybersecurity Framework Released</a></h2>\n",
        "        <span class=\"source\">Security Today</span>\n",
        "        <time datetime=\"2024-01-13\">January 13, 2024</time>\n",
        "        <p class=\"summary\">Government agencies collaborate on new standards...</p>\n",
        "    </article>\n",
        "    \n",
        "    <article class=\"news-item\" data-category=\"science\">\n",
        "        <h2 class=\"headline\"><a href=\"/article/4\">Mars Rover Discovers Ancient Riverbed</a></h2>\n",
        "        <span class=\"source\">Space News</span>\n",
        "        <time datetime=\"2024-01-12\">January 12, 2024</time>\n",
        "        <p class=\"summary\">NASA's Perseverance rover finds evidence of water...</p>\n",
        "    </article>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "print(\"ğŸ“° Sample News HTML loaded!\")\n",
        "print(\"   â€¢ 4 news articles\")\n",
        "print(\"   â€¢ Categories: technology, business, science\")\n",
        "print(\"   â€¢ Fields: headline, source, date, summary, link\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ± GROUP WORKSPACE: News Parser\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# \n",
        "# TODO: Create a NewsParser that extracts:\n",
        "# - Headline text\n",
        "# - Article URL (from the <a> inside headline)\n",
        "# - Category (from data-category attribute)\n",
        "# - Source (from span.source)\n",
        "# - Date (from time element)\n",
        "# - Summary (from p.summary)\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "# Your code here:\n",
        "pass  # Replace with your implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ¨ SOLUTION: News Headlines Parser\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Uncomment to see the solution!\n",
        "\n",
        "'''\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class NewsParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.articles = []\n",
        "        self.current_article = None\n",
        "        self.current_field = None\n",
        "        self.in_headline_link = False\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs_dict = dict(attrs)\n",
        "        class_name = attrs_dict.get('class', '')\n",
        "        \n",
        "        # Start of article\n",
        "        if tag == 'article' and 'news-item' in class_name:\n",
        "            category = attrs_dict.get('data-category', 'uncategorized')\n",
        "            self.current_article = {\n",
        "                'headline': None,\n",
        "                'url': None,\n",
        "                'category': category,\n",
        "                'source': None,\n",
        "                'date': None,\n",
        "                'summary': None\n",
        "            }\n",
        "        \n",
        "        elif self.current_article is not None:\n",
        "            # Headline link\n",
        "            if tag == 'a' and self.current_field == 'headline':\n",
        "                self.current_article['url'] = attrs_dict.get('href', '')\n",
        "                self.in_headline_link = True\n",
        "            \n",
        "            # Field detection\n",
        "            if tag == 'h2' and 'headline' in class_name:\n",
        "                self.current_field = 'headline'\n",
        "            elif tag == 'span' and 'source' in class_name:\n",
        "                self.current_field = 'source'\n",
        "            elif tag == 'time':\n",
        "                self.current_article['date'] = attrs_dict.get('datetime', '')\n",
        "            elif tag == 'p' and 'summary' in class_name:\n",
        "                self.current_field = 'summary'\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        text = data.strip()\n",
        "        if not text or not self.current_article:\n",
        "            return\n",
        "        \n",
        "        if self.in_headline_link:\n",
        "            self.current_article['headline'] = text\n",
        "        elif self.current_field == 'source':\n",
        "            self.current_article['source'] = text\n",
        "        elif self.current_field == 'summary':\n",
        "            self.current_article['summary'] = text\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        if tag == 'a' and self.in_headline_link:\n",
        "            self.in_headline_link = False\n",
        "        elif tag in ['h2', 'span', 'p']:\n",
        "            self.current_field = None\n",
        "        elif tag == 'article' and self.current_article:\n",
        "            self.articles.append(self.current_article)\n",
        "            self.current_article = None\n",
        "\n",
        "# Parse the news\n",
        "parser = NewsParser()\n",
        "parser.feed(news_html)  # Uses news_html from previous cell\n",
        "\n",
        "# Display results\n",
        "print(\"ğŸ“° News Headlines Extracted\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Group by category\n",
        "from collections import defaultdict\n",
        "by_category = defaultdict(list)\n",
        "for article in parser.articles:\n",
        "    by_category[article['category']].append(article)\n",
        "\n",
        "category_emojis = {\n",
        "    'technology': 'ğŸ’»',\n",
        "    'business': 'ğŸ’¼',\n",
        "    'science': 'ğŸ”¬',\n",
        "    'uncategorized': 'ğŸ“„'\n",
        "}\n",
        "\n",
        "for category, articles in sorted(by_category.items()):\n",
        "    emoji = category_emojis.get(category, 'ğŸ“„')\n",
        "    print(f\"\\\\n{emoji} {category.upper()}\")\n",
        "    print(\"-\" * 40)\n",
        "    for article in articles:\n",
        "        print(f\"  ğŸ“Œ {article['headline']}\")\n",
        "        print(f\"     {article['source']} | {article['date']}\")\n",
        "        print(f\"     ğŸ”— {article['url']}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(f\"ğŸ“Š Total: {len(parser.articles)} articles across {len(by_category)} categories\")\n",
        "'''\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the solution above to see it in action!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âš–ï¸ Activity 4: Ethics Discussion\n",
        "\n",
        "### Group Discussion: Responsible Web Scraping\n",
        "\n",
        "Discuss these scenarios with your team and decide: **Ethical or Not?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âš–ï¸ ETHICS SCENARIOS - Discuss with your team!\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "scenarios = [\n",
        "    {\n",
        "        \"scenario\": \"Scraping public product prices from Amazon for price comparison\",\n",
        "        \"considerations\": [\n",
        "            \"Amazon's Terms of Service prohibit scraping\",\n",
        "            \"They have a Product Advertising API you could use\",\n",
        "            \"Your script could be blocked or your IP banned\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Scraping Wikipedia for a research project\",\n",
        "        \"considerations\": [\n",
        "            \"Wikipedia allows scraping (check robots.txt)\",\n",
        "            \"They provide data dumps you can download instead\",\n",
        "            \"Be polite - add delays between requests\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Scraping LinkedIn profiles for recruiting\",\n",
        "        \"considerations\": [\n",
        "            \"LinkedIn explicitly prohibits scraping\",\n",
        "            \"Personal data has privacy implications (GDPR)\",\n",
        "            \"LinkedIn has sued companies for scraping\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Scraping government websites for public data\",\n",
        "        \"considerations\": [\n",
        "            \"Public data is often legally accessible\",\n",
        "            \"Many gov sites provide APIs or data downloads\",\n",
        "            \"Still respect rate limits to avoid disruption\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"Scraping a small blog for content to repost\",\n",
        "        \"considerations\": [\n",
        "            \"Copyright concerns - content belongs to creator\",\n",
        "            \"Even with attribution, reproduction may not be okay\",\n",
        "            \"Could harm the original site's SEO/traffic\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"âš–ï¸ Ethics Discussion Scenarios\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\\\nFor each scenario, discuss with your team:\")\n",
        "print(\"  1. Is this ethical?\")\n",
        "print(\"  2. Is this legal?\")\n",
        "print(\"  3. What would you do instead?\")\n",
        "print()\n",
        "\n",
        "for i, item in enumerate(scenarios, 1):\n",
        "    print(f\"\\\\n{'â”€' * 70}\")\n",
        "    print(f\"ğŸ“‹ SCENARIO {i}: {item['scenario']}\")\n",
        "    print(\"\\\\n   ğŸ¤” Considerations:\")\n",
        "    for c in item['considerations']:\n",
        "        print(f\"      â€¢ {c}\")\n",
        "    print()\n",
        "    print(\"   Your team's verdict: â“ (discuss!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“ Session Wrap-Up\n",
        "\n",
        "### ğŸ“‹ Collaboration Rubric\n",
        "\n",
        "Rate your team's collaboration (1-5 scale):\n",
        "\n",
        "| Criterion | Description | Score |\n",
        "|-----------|-------------|-------|\n",
        "| **Communication** | Did everyone participate in discussions? | /5 |\n",
        "| **Role Rotation** | Did team members try different roles? | /5 |\n",
        "| **Code Quality** | Is the code readable and documented? | /5 |\n",
        "| **Problem Solving** | Did the team work through challenges together? | /5 |\n",
        "| **Ethics Awareness** | Did the team consider ethical implications? | /5 |\n",
        "\n",
        "### ğŸ”‘ Key Takeaways\n",
        "\n",
        "| Concept | What We Learned |\n",
        "|---------|-----------------|\n",
        "| **HTMLParser** | Event-driven parsing with handlers |\n",
        "| **State Management** | Tracking context with class variables |\n",
        "| **URL Classification** | Absolute vs relative, internal vs external |\n",
        "| **Data Extraction** | Converting strings to proper types |\n",
        "| **Ethics** | Always check ToS, robots.txt, and be polite |\n",
        "\n",
        "### ğŸš¨ Web Scraping Best Practices\n",
        "\n",
        "```\n",
        "âœ… DO:\n",
        "   â€¢ Check robots.txt first\n",
        "   â€¢ Add 1+ second delays between requests\n",
        "   â€¢ Identify your bot in User-Agent\n",
        "   â€¢ Use APIs when available\n",
        "   â€¢ Handle errors gracefully\n",
        "   â€¢ Cache responses to avoid repeat requests\n",
        "\n",
        "âŒ DON'T:\n",
        "   â€¢ Ignore Terms of Service\n",
        "   â€¢ Overwhelm servers with requests\n",
        "   â€¢ Scrape personal/private data\n",
        "   â€¢ Bypass authentication\n",
        "   â€¢ Claim to be a real browser\n",
        "   â€¢ Ignore rate limiting (429 errors)\n",
        "```\n",
        "\n",
        "### ğŸ¯ Next Steps\n",
        "\n",
        "- Try scraping a simple, permitted website\n",
        "- Explore BeautifulSoup for easier parsing\n",
        "- Learn about Selenium for JavaScript sites\n",
        "- Build a small scraping project for your portfolio"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸŒ Week 8 â€” Introduction to Web Scraping\n",
        "\n",
        "## ğŸ¯ Session Overview\n",
        "\n",
        "Welcome to **Web Scraping**! This is where Python becomes your data-gathering superpower. Web scraping allows you to automatically extract data from websites - think of it as teaching Python to read and collect information from web pages just like you do.\n",
        "\n",
        "### What is Web Scraping?\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     WEB SCRAPING WORKFLOW                       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚   ğŸŒ Website  â†’  ğŸ“¥ Fetch HTML  â†’  ğŸ” Parse  â†’  ğŸ“Š Extract     â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚   example.com     HTTP Request      HTML         Structured     â”‚\n",
        "â”‚                   (urllib)          Parser       Data           â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "| Use Case | Example |\n",
        "|----------|---------|\n",
        "| ğŸ“Š **Data Collection** | Gathering product prices for comparison |\n",
        "| ğŸ“° **News Aggregation** | Collecting headlines from multiple sources |\n",
        "| ğŸ  **Real Estate** | Monitoring property listings |\n",
        "| ğŸ’¼ **Job Hunting** | Scraping job postings |\n",
        "| ğŸ“ˆ **Research** | Collecting academic data |\n",
        "\n",
        "### âš ï¸ Important: Ethics & Legality\n",
        "\n",
        "| Do âœ… | Don't âŒ |\n",
        "|------|---------|\n",
        "| Check robots.txt | Ignore rate limits |\n",
        "| Respect Terms of Service | Scrape personal data |\n",
        "| Add delays between requests | Overwhelm servers |\n",
        "| Use APIs when available | Bypass authentication |\n",
        "| Identify your bot politely | Claim to be a browser |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "\n",
        "- âœ… **Fetch** web pages using Python's `urllib` standard library\n",
        "- âœ… **Parse** HTML content using `html.parser` from the standard library\n",
        "- âœ… **Extract** links and data from HTML documents\n",
        "- âœ… **Handle** common web scraping errors and edge cases\n",
        "- âœ… **Understand** ethical considerations and best practices\n",
        "- âœ… **Apply** web scraping techniques to gather real data\n",
        "\n",
        "### ğŸ“š Topics Covered\n",
        "\n",
        "| Section | Topic | Key Concept |\n",
        "|---------|-------|-------------|\n",
        "| 1 | HTTP Basics | How web requests work |\n",
        "| 2 | Fetching Pages | urllib.request |\n",
        "| 3 | Parsing HTML | html.parser |\n",
        "| 4 | Extracting Data | Finding specific elements |\n",
        "| 5 | Error Handling | Robust scraping |\n",
        "| 6 | Best Practices | Ethical scraping |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### ğŸ”„ JavaScript Transition Note\n",
        "\n",
        "If you're coming from JavaScript/web development:\n",
        "\n",
        "| JavaScript | Python | Purpose |\n",
        "|------------|--------|---------|\n",
        "| `fetch()` / `axios` | `urllib.request` | Make HTTP requests |\n",
        "| `document.querySelector()` | `html.parser` | Parse HTML |\n",
        "| `element.getAttribute()` | Handler methods | Extract attributes |\n",
        "| `JSON.parse()` | `json.loads()` | Parse JSON responses |\n",
        "\n",
        "**Key Differences:**\n",
        "- Python's `html.parser` uses an **event-driven** approach (callbacks for tags)\n",
        "- JavaScript's DOM is **tree-based** (query selectors)\n",
        "- Both achieve the same goal: extracting data from HTML!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¡ Section 1: Understanding HTTP Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“¡ HTTP REQUEST BASICS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Before we scrape, let's understand what happens when you visit a website:\n",
        "\n",
        "print(\"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        HTTP REQUEST FLOW                                   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                            â”‚\n",
        "â”‚  Your Code              Request               Web Server                   â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚  â”‚ Python  â”‚    GET /page HTTP/1.1           â”‚   Server    â”‚              â”‚\n",
        "â”‚  â”‚ Script  â”‚    Host: example.com            â”‚  (Apache,   â”‚              â”‚\n",
        "â”‚  â”‚         â”‚    User-Agent: Python           â”‚   Nginx)    â”‚              â”‚\n",
        "â”‚  â”‚         â”‚                                 â”‚             â”‚              â”‚\n",
        "â”‚  â”‚         â”‚    <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚             â”‚              â”‚\n",
        "â”‚  â”‚         â”‚    HTTP/1.1 200 OK              â”‚             â”‚              â”‚\n",
        "â”‚  â”‚         â”‚    Content-Type: text/html      â”‚             â”‚              â”‚\n",
        "â”‚  â”‚         â”‚    <html>...</html>             â”‚             â”‚              â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
        "â”‚                                                                            â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "\n",
        "print(\"ğŸ“‹ Common HTTP Status Codes:\")\n",
        "print(\"=\" * 50)\n",
        "status_codes = {\n",
        "    200: (\"OK\", \"Request successful\"),\n",
        "    301: (\"Moved Permanently\", \"Page has moved\"),\n",
        "    403: (\"Forbidden\", \"Access denied\"),\n",
        "    404: (\"Not Found\", \"Page doesn't exist\"),\n",
        "    429: (\"Too Many Requests\", \"Rate limited!\"),\n",
        "    500: (\"Server Error\", \"Website problem\")\n",
        "}\n",
        "for code, (name, desc) in status_codes.items():\n",
        "    emoji = \"âœ…\" if code == 200 else \"âš ï¸\" if code < 500 else \"âŒ\"\n",
        "    print(f\"  {emoji} {code}: {name} - {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“¥ Section 2: Fetching Web Pages with urllib\n",
        "\n",
        "The `urllib` module is Python's built-in tool for making HTTP requests. No installation needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸš« BAD PRACTICE: Fetching without error handling\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âŒ Problem: No error handling, no timeout, crashes on failure\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# This will crash if the URL is invalid or network fails!\n",
        "# response = urlopen(\"https://definitely-not-a-real-website-12345.com\")\n",
        "\n",
        "# Let's simulate with a simple HTML string instead\n",
        "html = '<html><body><a href=\"https://example.com\">example</a></body></html>'\n",
        "print(f\"HTML length: {len(html)} characters\")\n",
        "\n",
        "# âš ï¸ What's wrong here:\n",
        "# - No try/except for network errors\n",
        "# - No timeout (could hang forever)\n",
        "# - No status code checking\n",
        "# - No User-Agent (might be blocked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ± NOVICE: Basic fetch with error handling\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Better: Added try/except for errors\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import URLError, HTTPError\n",
        "\n",
        "def fetch_page(url):\n",
        "    \"\"\"Fetch a web page and return its content.\"\"\"\n",
        "    try:\n",
        "        with urlopen(url) as response:\n",
        "            return response.read().decode('utf-8')\n",
        "    except HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.code}\")\n",
        "        return None\n",
        "    except URLError as e:\n",
        "        print(f\"URL Error: {e.reason}\")\n",
        "        return None\n",
        "\n",
        "# Test with a sample (we'll use embedded HTML for safety)\n",
        "sample_html = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head><title>Sample Page</title></head>\n",
        "<body>\n",
        "    <h1>Welcome!</h1>\n",
        "    <a href=\"https://example.com\">Example Link</a>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "print(\"âœ… Function defined! Here's our sample HTML:\")\n",
        "print(\"=\" * 50)\n",
        "print(sample_html)\n",
        "print(\"=\" * 50)\n",
        "print(f\"Length: {len(sample_html)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ”§ INTERMEDIATE: Fetch with timeout and headers\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Better: Added timeout, User-Agent header, status code checking\n",
        "\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.error import URLError, HTTPError\n",
        "\n",
        "def fetch_with_headers(url, timeout=10):\n",
        "    \"\"\"\n",
        "    Fetch a web page with proper headers and timeout.\n",
        "    \n",
        "    Args:\n",
        "        url: The URL to fetch\n",
        "        timeout: Max seconds to wait (default 10)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (content, status_code) or (None, error_code)\n",
        "    \"\"\"\n",
        "    # Add a User-Agent to be polite and avoid blocks\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Educational Python Script)',\n",
        "        'Accept': 'text/html'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        request = Request(url, headers=headers)\n",
        "        with urlopen(request, timeout=timeout) as response:\n",
        "            content = response.read().decode('utf-8')\n",
        "            return (content, response.status)\n",
        "    except HTTPError as e:\n",
        "        print(f\"âš ï¸ HTTP Error {e.code}: {e.reason}\")\n",
        "        return (None, e.code)\n",
        "    except URLError as e:\n",
        "        print(f\"âŒ URL Error: {e.reason}\")\n",
        "        return (None, -1)\n",
        "    except TimeoutError:\n",
        "        print(f\"â° Timeout after {timeout} seconds\")\n",
        "        return (None, -1)\n",
        "\n",
        "# Demonstration (without actual network call)\n",
        "print(\"âœ… fetch_with_headers() function defined!\")\n",
        "print()\n",
        "print(\"ğŸ“ Features added:\")\n",
        "print(\"  â€¢ Custom User-Agent header (identifies your bot)\")\n",
        "print(\"  â€¢ Accept header (tells server what we want)\")\n",
        "print(\"  â€¢ Timeout (prevents hanging)\")\n",
        "print(\"  â€¢ Returns status code for checking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ¨ PYTHONIC: Complete web fetcher class with rate limiting\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Best: Rate limiting, retry logic, robots.txt checking\n",
        "\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.error import URLError, HTTPError\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "\n",
        "class WebFetcher:\n",
        "    \"\"\"\n",
        "    A polite web fetcher with rate limiting and error handling.\n",
        "    \n",
        "    Features:\n",
        "    - Rate limiting (respects servers)\n",
        "    - Retry logic (handles temporary failures)\n",
        "    - Custom headers (identifies your bot)\n",
        "    - Timeout protection (prevents hanging)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, delay=1.0, retries=3, timeout=10):\n",
        "        \"\"\"\n",
        "        Initialize the fetcher.\n",
        "        \n",
        "        Args:\n",
        "            delay: Seconds between requests (be polite!)\n",
        "            retries: Number of retry attempts\n",
        "            timeout: Request timeout in seconds\n",
        "        \"\"\"\n",
        "        self.delay = delay\n",
        "        self.retries = retries\n",
        "        self.timeout = timeout\n",
        "        self.last_request_time = 0\n",
        "        self.headers = {\n",
        "            'User-Agent': 'EducationalBot/1.0 (Learning Python)',\n",
        "            'Accept': 'text/html,application/xhtml+xml',\n",
        "            'Accept-Language': 'en-US,en;q=0.9'\n",
        "        }\n",
        "    \n",
        "    def _wait_for_rate_limit(self):\n",
        "        \"\"\"Wait if needed to respect rate limit.\"\"\"\n",
        "        elapsed = time.time() - self.last_request_time\n",
        "        if elapsed < self.delay:\n",
        "            wait_time = self.delay - elapsed\n",
        "            print(f\"â³ Rate limiting: waiting {wait_time:.2f}s...\")\n",
        "            time.sleep(wait_time)\n",
        "        self.last_request_time = time.time()\n",
        "    \n",
        "    def fetch(self, url):\n",
        "        \"\"\"\n",
        "        Fetch a URL with retries and rate limiting.\n",
        "        \n",
        "        Returns:\n",
        "            dict with 'content', 'status', 'success' keys\n",
        "        \"\"\"\n",
        "        self._wait_for_rate_limit()\n",
        "        \n",
        "        for attempt in range(1, self.retries + 1):\n",
        "            try:\n",
        "                request = Request(url, headers=self.headers)\n",
        "                with urlopen(request, timeout=self.timeout) as response:\n",
        "                    content = response.read().decode('utf-8')\n",
        "                    return {\n",
        "                        'content': content,\n",
        "                        'status': response.status,\n",
        "                        'success': True,\n",
        "                        'url': url\n",
        "                    }\n",
        "                    \n",
        "            except HTTPError as e:\n",
        "                print(f\"  âš ï¸ Attempt {attempt}: HTTP {e.code}\")\n",
        "                if e.code == 429:  # Rate limited\n",
        "                    time.sleep(5 * attempt)  # Back off\n",
        "                elif e.code >= 400:\n",
        "                    return {'content': None, 'status': e.code, 'success': False, 'url': url}\n",
        "                    \n",
        "            except URLError as e:\n",
        "                print(f\"  âŒ Attempt {attempt}: {e.reason}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ Attempt {attempt}: {e}\")\n",
        "            \n",
        "            if attempt < self.retries:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        \n",
        "        return {'content': None, 'status': -1, 'success': False, 'url': url}\n",
        "\n",
        "# Demonstration\n",
        "fetcher = WebFetcher(delay=0.5, retries=2, timeout=5)\n",
        "print(\"âœ… WebFetcher class defined!\")\n",
        "print()\n",
        "print(\"ğŸ“Š Configuration:\")\n",
        "print(f\"  â€¢ Delay between requests: {fetcher.delay}s\")\n",
        "print(f\"  â€¢ Retry attempts: {fetcher.retries}\")\n",
        "print(f\"  â€¢ Timeout: {fetcher.timeout}s\")\n",
        "print()\n",
        "print(\"ğŸ¨ Usage example:\")\n",
        "print(\"  fetcher = WebFetcher()\")\n",
        "print('  result = fetcher.fetch(\"https://example.com\")')\n",
        "print('  if result[\"success\"]:')\n",
        "print('      html = result[\"content\"]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ” Section 3: Parsing HTML with html.parser\n",
        "\n",
        "Now that we can fetch pages, let's extract data! The `html.parser` module uses an **event-driven** approach - you define handlers that get called when the parser encounters different HTML elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸš« BAD PRACTICE: Using string methods to parse HTML\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âŒ Problem: String methods break with complex HTML!\n",
        "\n",
        "html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <a href=\"https://example.com\">Example</a>\n",
        "    <a href='https://test.com'>Test</a>\n",
        "    <a \n",
        "       href=\"https://multiline.com\">Multiline</a>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# This approach is FRAGILE and will miss many links!\n",
        "links = []\n",
        "for line in html.split('\\n'):\n",
        "    if 'href=\"' in line:\n",
        "        start = line.find('href=\"') + 6\n",
        "        end = line.find('\"', start)\n",
        "        if end > start:\n",
        "            links.append(line[start:end])\n",
        "\n",
        "print(\"ğŸš« Links found with string parsing:\")\n",
        "print(links)\n",
        "print()\n",
        "print(\"âš ï¸ Problems with this approach:\")\n",
        "print(\"  âŒ Misses single-quoted attributes\")\n",
        "print(\"  âŒ Breaks with multiline attributes\")\n",
        "print(\"  âŒ Can't handle nested tags\")\n",
        "print(\"  âŒ No encoding handling\")\n",
        "print(\"  âŒ Regex would be even worse!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ± NOVICE: Basic HTMLParser for links\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Better: Properly parses HTML structure\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class SimpleLinkParser(HTMLParser):\n",
        "    \"\"\"A simple parser that extracts all links.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.links = []\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        \"\"\"Called when parser finds an opening tag like <a>.\"\"\"\n",
        "        if tag == 'a':\n",
        "            for name, value in attrs:\n",
        "                if name == 'href':\n",
        "                    self.links.append(value)\n",
        "\n",
        "# Test HTML (same as before)\n",
        "html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <a href=\"https://example.com\">Example</a>\n",
        "    <a href='https://test.com'>Test</a>\n",
        "    <a \n",
        "       href=\"https://multiline.com\">Multiline</a>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Parse it!\n",
        "parser = SimpleLinkParser()\n",
        "parser.feed(html)\n",
        "\n",
        "print(\"âœ… Links found with HTMLParser:\")\n",
        "for link in parser.links:\n",
        "    print(f\"  â€¢ {link}\")\n",
        "\n",
        "print()\n",
        "print(\"ğŸ’¡ HTMLParser handles:\")\n",
        "print(\"  âœ… Single and double quotes\")\n",
        "print(\"  âœ… Multiline attributes\")\n",
        "print(\"  âœ… Proper HTML structure\")\n",
        "print(\"  âœ… Character encoding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Section 4: Extracting Specific Data\n",
        "\n",
        "Let's build more sophisticated parsers that can extract specific types of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ”§ INTERMEDIATE: Parser for links with text content\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Better: Capture both URL and link text\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class LinkTextParser(HTMLParser):\n",
        "    \"\"\"Parser that extracts links with their text content.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.links = []         # List of (url, text) tuples\n",
        "        self.current_link = None  # Currently parsing link\n",
        "        self.current_text = \"\"    # Text inside current link\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        if tag == 'a':\n",
        "            for name, value in attrs:\n",
        "                if name == 'href':\n",
        "                    self.current_link = value\n",
        "                    self.current_text = \"\"\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        \"\"\"Called for text content between tags.\"\"\"\n",
        "        if self.current_link is not None:\n",
        "            self.current_text += data.strip()\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        \"\"\"Called for closing tags like </a>.\"\"\"\n",
        "        if tag == 'a' and self.current_link:\n",
        "            self.links.append((self.current_link, self.current_text))\n",
        "            self.current_link = None\n",
        "            self.current_text = \"\"\n",
        "\n",
        "# Sample HTML with links\n",
        "html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <nav>\n",
        "        <a href=\"/\">Home</a>\n",
        "        <a href=\"/about\">About Us</a>\n",
        "        <a href=\"/products\">Our Products</a>\n",
        "        <a href=\"https://external.com\">External Link</a>\n",
        "    </nav>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "parser = LinkTextParser()\n",
        "parser.feed(html)\n",
        "\n",
        "print(\"ğŸ”— Links with Text Content:\")\n",
        "print(\"=\" * 50)\n",
        "for url, text in parser.links:\n",
        "    link_type = \"ğŸ”— External\" if url.startswith('http') else \"ğŸ“„ Internal\"\n",
        "    print(f\"  {link_type} '{text}' â†’ {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ¨ PYTHONIC: Complete data extractor with multiple elements\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… Best: Extract headings, links, paragraphs, and images\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "class PageDataExtractor(HTMLParser):\n",
        "    \"\"\"\n",
        "    Comprehensive HTML parser that extracts structured data.\n",
        "    \n",
        "    Extracts:\n",
        "    - Page title\n",
        "    - All headings (h1-h6)\n",
        "    - All links with text\n",
        "    - All images with alt text\n",
        "    - All paragraphs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_url=\"\"):\n",
        "        super().__init__()\n",
        "        self.base_url = base_url\n",
        "        self.data = {\n",
        "            'title': '',\n",
        "            'headings': [],\n",
        "            'links': [],\n",
        "            'images': [],\n",
        "            'paragraphs': []\n",
        "        }\n",
        "        \n",
        "        # State tracking\n",
        "        self._current_tag = None\n",
        "        self._current_attrs = {}\n",
        "        self._current_text = \"\"\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs_dict = dict(attrs)\n",
        "        self._current_tag = tag\n",
        "        self._current_attrs = attrs_dict\n",
        "        self._current_text = \"\"\n",
        "        \n",
        "        # Handle self-closing tags\n",
        "        if tag == 'img':\n",
        "            src = attrs_dict.get('src', '')\n",
        "            alt = attrs_dict.get('alt', 'No alt text')\n",
        "            if src:\n",
        "                full_url = urljoin(self.base_url, src)\n",
        "                self.data['images'].append({'src': full_url, 'alt': alt})\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        if self._current_tag:\n",
        "            self._current_text += data.strip()\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        text = self._current_text.strip()\n",
        "        \n",
        "        if tag == 'title' and text:\n",
        "            self.data['title'] = text\n",
        "            \n",
        "        elif tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and text:\n",
        "            self.data['headings'].append({\n",
        "                'level': int(tag[1]),\n",
        "                'text': text\n",
        "            })\n",
        "            \n",
        "        elif tag == 'a' and text:\n",
        "            href = self._current_attrs.get('href', '')\n",
        "            if href:\n",
        "                full_url = urljoin(self.base_url, href)\n",
        "                self.data['links'].append({\n",
        "                    'url': full_url,\n",
        "                    'text': text,\n",
        "                    'external': full_url.startswith('http') and self.base_url not in full_url\n",
        "                })\n",
        "                \n",
        "        elif tag == 'p' and text:\n",
        "            self.data['paragraphs'].append(text)\n",
        "        \n",
        "        self._current_tag = None\n",
        "        self._current_text = \"\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“„ Sample Complex HTML Page\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "sample_page = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Python Tutorial Site</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Welcome to Python Learning</h1>\n",
        "    \n",
        "    <nav>\n",
        "        <a href=\"/\">Home</a>\n",
        "        <a href=\"/tutorials\">Tutorials</a>\n",
        "        <a href=\"https://python.org\">Python.org</a>\n",
        "    </nav>\n",
        "    \n",
        "    <main>\n",
        "        <h2>Getting Started</h2>\n",
        "        <p>Python is a versatile programming language perfect for beginners.</p>\n",
        "        \n",
        "        <img src=\"/images/python-logo.png\" alt=\"Python Logo\">\n",
        "        \n",
        "        <h2>Popular Topics</h2>\n",
        "        <p>Learn about data types, functions, and classes.</p>\n",
        "        \n",
        "        <h3>Data Types</h3>\n",
        "        <p>Strings, integers, floats, and more!</p>\n",
        "    </main>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Parse the page\n",
        "extractor = PageDataExtractor(base_url=\"https://example.com\")\n",
        "extractor.feed(sample_page)\n",
        "\n",
        "# Display results\n",
        "print(\"ğŸ“„ Page Analysis Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nğŸ“Œ Title: {extractor.data['title']}\")\n",
        "\n",
        "print(f\"\\nğŸ“‘ Headings ({len(extractor.data['headings'])}):\")\n",
        "for h in extractor.data['headings']:\n",
        "    indent = \"  \" * h['level']\n",
        "    print(f\"  {'#' * h['level']} {h['text']}\")\n",
        "\n",
        "print(f\"\\nğŸ”— Links ({len(extractor.data['links'])}):\")\n",
        "for link in extractor.data['links']:\n",
        "    icon = \"ğŸŒ\" if link['external'] else \"ğŸ“„\"\n",
        "    print(f\"  {icon} {link['text']} â†’ {link['url']}\")\n",
        "\n",
        "print(f\"\\nğŸ–¼ï¸ Images ({len(extractor.data['images'])}):\")\n",
        "for img in extractor.data['images']:\n",
        "    print(f\"  â€¢ {img['alt']} ({img['src']})\")\n",
        "\n",
        "print(f\"\\nğŸ“ Paragraphs ({len(extractor.data['paragraphs'])}):\")\n",
        "for i, p in enumerate(extractor.data['paragraphs'], 1):\n",
        "    preview = p[:50] + \"...\" if len(p) > 50 else p\n",
        "    print(f\"  {i}. {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Section 5: Practice Challenges\n",
        "\n",
        "Try these exercises to reinforce your web scraping skills!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ¯ CHALLENGE 1: Extract Absolute URLs Only\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Task: Modify the LinkParser to only collect absolute URLs (starting with http)\n",
        "\n",
        "html_challenge = '''\n",
        "<html>\n",
        "<body>\n",
        "    <a href=\"https://google.com\">Google</a>\n",
        "    <a href=\"/local/page\">Local Page</a>\n",
        "    <a href=\"http://example.org\">Example</a>\n",
        "    <a href=\"../relative/path\">Relative</a>\n",
        "    <a href=\"https://python.org\">Python</a>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# TODO: Create a parser that only collects absolute URLs\n",
        "# Expected output: ['https://google.com', 'http://example.org', 'https://python.org']\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class AbsoluteURLParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.absolute_links = []\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        # TODO: Implement this!\n",
        "        # Hint: Check if href starts with 'http'\n",
        "        pass\n",
        "\n",
        "# Your code here:\n",
        "# parser = AbsoluteURLParser()\n",
        "# parser.feed(html_challenge)\n",
        "# print(parser.absolute_links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… SOLUTION: Challenge 1 - Absolute URLs Only\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class AbsoluteURLParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.absolute_links = []\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        if tag == 'a':\n",
        "            for name, value in attrs:\n",
        "                if name == 'href' and value.startswith(('http://', 'https://')):\n",
        "                    self.absolute_links.append(value)\n",
        "\n",
        "# Test it\n",
        "html_challenge = '''\n",
        "<html>\n",
        "<body>\n",
        "    <a href=\"https://google.com\">Google</a>\n",
        "    <a href=\"/local/page\">Local Page</a>\n",
        "    <a href=\"http://example.org\">Example</a>\n",
        "    <a href=\"../relative/path\">Relative</a>\n",
        "    <a href=\"https://python.org\">Python</a>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "parser = AbsoluteURLParser()\n",
        "parser.feed(html_challenge)\n",
        "\n",
        "print(\"âœ… Absolute URLs Found:\")\n",
        "for url in parser.absolute_links:\n",
        "    print(f\"  ğŸŒ {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ¯ CHALLENGE 2: Extract Product Data\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Task: Parse a product listing page and extract product info\n",
        "\n",
        "product_html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">Laptop Pro</h2>\n",
        "        <span class=\"price\">$999</span>\n",
        "        <p class=\"description\">High-performance laptop</p>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">Wireless Mouse</h2>\n",
        "        <span class=\"price\">$29</span>\n",
        "        <p class=\"description\">Ergonomic design</p>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">USB Keyboard</h2>\n",
        "        <span class=\"price\">$49</span>\n",
        "        <p class=\"description\">Mechanical keys</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# TODO: Create a parser that extracts:\n",
        "# - Product names (from h2.name)\n",
        "# - Prices (from span.price)\n",
        "# - Descriptions (from p.description)\n",
        "#\n",
        "# Expected output: List of dicts with 'name', 'price', 'description'\n",
        "\n",
        "# Hint: Track which class you're inside using handle_starttag attrs\n",
        "# and use a state variable to know what type of data you're collecting\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âœ… SOLUTION: Challenge 2 - Product Data Extractor\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class ProductParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.products = []\n",
        "        self.current_product = {}\n",
        "        self.current_field = None\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs_dict = dict(attrs)\n",
        "        class_name = attrs_dict.get('class', '')\n",
        "        \n",
        "        if tag == 'div' and class_name == 'product':\n",
        "            self.current_product = {}\n",
        "        elif tag == 'h2' and class_name == 'name':\n",
        "            self.current_field = 'name'\n",
        "        elif tag == 'span' and class_name == 'price':\n",
        "            self.current_field = 'price'\n",
        "        elif tag == 'p' and class_name == 'description':\n",
        "            self.current_field = 'description'\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        if self.current_field and data.strip():\n",
        "            self.current_product[self.current_field] = data.strip()\n",
        "            self.current_field = None\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        if tag == 'div' and self.current_product:\n",
        "            self.products.append(self.current_product)\n",
        "            self.current_product = {}\n",
        "\n",
        "# Test it\n",
        "product_html = '''\n",
        "<html>\n",
        "<body>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">Laptop Pro</h2>\n",
        "        <span class=\"price\">$999</span>\n",
        "        <p class=\"description\">High-performance laptop</p>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">Wireless Mouse</h2>\n",
        "        <span class=\"price\">$29</span>\n",
        "        <p class=\"description\">Ergonomic design</p>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <h2 class=\"name\">USB Keyboard</h2>\n",
        "        <span class=\"price\">$49</span>\n",
        "        <p class=\"description\">Mechanical keys</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "parser = ProductParser()\n",
        "parser.feed(product_html)\n",
        "\n",
        "print(\"ğŸ›’ Products Found:\")\n",
        "print(\"=\" * 60)\n",
        "for i, product in enumerate(parser.products, 1):\n",
        "    print(f\"\\nğŸ“¦ Product {i}:\")\n",
        "    print(f\"   Name: {product.get('name', 'N/A')}\")\n",
        "    print(f\"   Price: {product.get('price', 'N/A')}\")\n",
        "    print(f\"   Description: {product.get('description', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š Session Recap\n",
        "\n",
        "### What We Learned Today:\n",
        "\n",
        "| Topic | Key Concept | Tool |\n",
        "|-------|-------------|------|\n",
        "| **HTTP Requests** | Fetching web pages | `urllib.request` |\n",
        "| **Error Handling** | Try/except for network errors | `URLError`, `HTTPError` |\n",
        "| **HTML Parsing** | Event-driven parsing | `html.parser.HTMLParser` |\n",
        "| **Data Extraction** | Custom parser classes | `handle_starttag()`, `handle_data()` |\n",
        "| **Best Practices** | Rate limiting, retries | Custom `WebFetcher` class |\n",
        "\n",
        "### The Evolution of Our Code:\n",
        "\n",
        "| Level | Characteristics |\n",
        "|-------|-----------------|\n",
        "| ğŸš« **Bad** | No error handling, string parsing |\n",
        "| ğŸŒ± **Novice** | Basic try/except, simple parser |\n",
        "| ğŸ”§ **Intermediate** | Headers, timeout, link+text extraction |\n",
        "| âœ¨ **Pythonic** | Rate limiting, retries, comprehensive extraction |\n",
        "\n",
        "### Key HTMLParser Methods:\n",
        "\n",
        "```python\n",
        "class MyParser(HTMLParser):\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        # Called for <tag attr=\"value\">\n",
        "        pass\n",
        "    \n",
        "    def handle_data(self, data):\n",
        "        # Called for text content\n",
        "        pass\n",
        "    \n",
        "    def handle_endtag(self, tag):\n",
        "        # Called for </tag>\n",
        "        pass\n",
        "```\n",
        "\n",
        "### ğŸš¨ Ethical Web Scraping Checklist:\n",
        "\n",
        "- [ ] âœ… Check robots.txt before scraping\n",
        "- [ ] âœ… Add delays between requests (1+ second)\n",
        "- [ ] âœ… Identify your bot with User-Agent\n",
        "- [ ] âœ… Handle errors gracefully\n",
        "- [ ] âœ… Respect rate limits (429 status)\n",
        "- [ ] âœ… Don't scrape personal data\n",
        "- [ ] âœ… Use APIs when available\n",
        "\n",
        "### ğŸ¯ Next Steps:\n",
        "\n",
        "- Try scraping a real (permitted) website\n",
        "- Explore BeautifulSoup for easier parsing\n",
        "- Learn about Selenium for JavaScript-heavy sites\n",
        "- Practice with the group session activities!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
